{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaifaliKhulbe/Project_1/blob/main/Full_final_tweets%2Bretweets%2Bquote_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pHctLtkbXszt"
      },
      "outputs": [],
      "source": [
        "# Import the libraries.\n",
        "import os\n",
        "import tweepy as tw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import configparser\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read('Config_2.ini')\n",
        "\n",
        "\n",
        "# Assigning Twitter Developer API’s and Access Tokens to variables.\n",
        "api_key = config['twitter']['api_key']\n",
        "api_secret = config['twitter']['api_key_secret']\n",
        "\n",
        "access_token = config['twitter']['access_token']\n",
        "access_token_secret = config['twitter']['access_token_secret'] \n",
        "\n",
        "\n",
        "# Authenticate the API’s and Access Tokens.\n",
        "auth = tw.OAuthHandler(api_key, api_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=True)\n",
        "\n",
        "# Define the search words and start date as variables. \n",
        "#search_words = \"football\"\n",
        "date_since = \"2022-01-01\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quote_tweets(name, tweet_id, tweet_url):\n",
        "\n",
        "\n",
        "  replies=[] #replies to original tweet\n",
        "  quotes=[] #tweets with true quote status\n",
        "  matching_quotes = [] #tweet\n",
        "  quote_ids = [] #tweet id of tweet being quoted\n",
        "  objects = [] #tweet metadata\n",
        "  all_tweets=[] #all tweets returned\n",
        "  for tweet in tw.Cursor(api.search,q=tweet_url).items(3000):\n",
        "    #Checks if tweet is a tweet reply to the specific tweet\n",
        "      all_tweets.append(tweet)\n",
        "      if hasattr(tweet, 'in_reply_to_status_id_str'):\n",
        "          if (tweet.in_reply_to_status_id_str==tweet_id):\n",
        "              replies.append(tweet)\n",
        "      #if hasattr(tweet, 'quoted_status'):\n",
        "      if tweet.is_quote_status == True:\n",
        "        quotes.append(tweet) #Full Tweet objects with all the meta data\n",
        "        quote_ids.append(tweet.quoted_status_id) #These should all be the exact same\n",
        "        if str(tweet_id) == str(tweet.quoted_status_id):\n",
        "          objects.append(tweet)\n",
        "          matching_quotes.append(tweet.text)\n",
        "      \n",
        "\n",
        "  return objects"
      ],
      "metadata": {
        "id": "7Ef0KNIpXzXT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources: \n",
        "\n",
        "https://colab.research.google.com/drive/1jLOCy0LdK396lTth44uTqNmV3WbaAQ4f?usp=sharing#scrollTo=2eyHQIWn2t1w"
      ],
      "metadata": {
        "id": "PmUrP5iOf0HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_related_tweets(key_word, date_since):\n",
        "\n",
        "\n",
        "    tweet_user = []\n",
        "    tweet_time = []\n",
        "    tweet_string = []\n",
        "    tweet_location = [] \n",
        "    tweet_followers = []\n",
        "    tweet_following = []\n",
        "    tweet_like = []\n",
        "    tweet_hashtags = []\n",
        "    tweet_description = []\n",
        "    tweet_totaltweets = []\n",
        "    tweet_retweet_count = []\n",
        "    tweet_id = []\n",
        "    tweet_place = []\n",
        "    \n",
        "    \n",
        "\n",
        "    retweet_user = []\n",
        "    retweet_time = []\n",
        "    retweet_location = [] \n",
        "    retweet_followers = []\n",
        "    retweet_following = []\n",
        "    retweet_hashtags = []\n",
        "    retweet_description = []\n",
        "    retweet_string = []\n",
        "    retweet_id = []\n",
        "    retweet_place = []\n",
        "    \n",
        "    \n",
        "    quote_tweet_user = []\n",
        "    quote_tweet_time = []\n",
        "    quote_tweet_location = [] \n",
        "    quote_tweet_followers = []\n",
        "    quote_tweet_following = []\n",
        "    quote_tweet_hashtags = []\n",
        "    quote_tweet_description = []\n",
        "    quote_tweet_string = []\n",
        "    quote_tweet_id = []\n",
        "    quote_tweet_place = []\n",
        "    \n",
        "\n",
        "\n",
        "    for tweet in tqdm(tw.Cursor(api.search, lang = \"en\", q=key_word, since = date_since, count=50, tweet_mode='extended').items(50)):\n",
        "            if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
        "                if tweet.lang == \"en\":\n",
        "\n",
        "                    #COLLECTING INFO ON TWEETS\n",
        "\n",
        "                    tweet_user.append(tweet.user.name)\n",
        "                    tweet_time.append(tweet.created_at)\n",
        "                    tweet_string.append(tweet.full_text)\n",
        "                    tweet_location.append(tweet.user.location)\n",
        "                    tweet_place.append(tweet.place)\n",
        "                    \n",
        "                    tweet_followers.append(tweet.user.followers_count)\n",
        "                    tweet_following.append(tweet.user.friends_count)\n",
        "                    tweet_like.append(tweet.favorite_count)           \n",
        "                    tweet_id.append(tweet.id_str)\n",
        "                    tweet_description.append(tweet.user.description)\n",
        "                    tweet_totaltweets.append(tweet.user.statuses_count)\n",
        "                    tweet_retweet_count.append(tweet.retweet_count)\n",
        "\n",
        "                    #extract hashtags\n",
        "                    hashtags = tweet.entities['hashtags']\n",
        "                    hashtext = list()\n",
        "                    for i in range(0, len(hashtags)):\n",
        "                      hashtext.append(hashtags[i]['text'])\n",
        "                    tweet_hashtags.append(hashtext)\n",
        "\n",
        "\n",
        "                    #COLLECTING INFO ON RETWEETS\n",
        "\n",
        "                    retweeters = api.retweets(tweet.id_str)\n",
        "\n",
        "                    for retweet in retweeters:                              \n",
        "                      \n",
        "                      #original tweet's id which has been retweeted\n",
        "                      retweet_id.append(tweet.id_str)\n",
        "                      retweet_string.append(retweet.text)              \n",
        "                      retweet_user.append(retweet.user.name)\n",
        "                      retweet_time.append(retweet.created_at)\n",
        "                      retweet_description.append(retweet.user.description)\n",
        "                      retweet_location.append(retweet.user.location)\n",
        "                      retweet_followers.append(retweet.user.followers_count)\n",
        "                      retweet_following.append(retweet.user.friends_count)\n",
        "                      retweet_place.append(retweet.place)\n",
        "                      \n",
        "                      \n",
        "                      \n",
        "                      #extract hashtags\n",
        "                      hashtags = retweet.entities['hashtags']\n",
        "                      hashtext = list()\n",
        "                      for i in range(0, len(hashtags)):\n",
        "                        hashtext.append(hashtags[i]['text'])\n",
        "                      retweet_hashtags.append(hashtext)\n",
        "\n",
        "\n",
        "                    #COLLECTING INFO ON QUOTE TWEETS\n",
        "\n",
        "                    tweet_url = f\"https://twitter.com/{tweet.user.screen_name}/status/{tweet.id}\"\n",
        "\n",
        "\n",
        "                    objects = quote_tweets(tweet.user.name , tweet.id_str, tweet_url)\n",
        "\n",
        "                    print(\"Number of quote tweets: \", len(objects))\n",
        "\n",
        "                    for qtweet in objects:\n",
        "                        \n",
        "                      print(\"TWEET.ID: \" , tweet.id_str)\n",
        "                      print(\"TWEET.TEXT: \", tweet.text)\n",
        "                      print(qtweet.text)\n",
        "                      print(qtweet.created_at)\n",
        "\n",
        "    \n",
        "                      quote_tweet_id.append(tweet.id_str)\n",
        "                      quote_tweet_string.append(qtweet.full_text)              \n",
        "                      quote_tweet_user.append(qtweet.user.name)\n",
        "                      quote_tweet_time.append(qtweet.created_at)\n",
        "                      quote_tweet_description.append(qtweet.user.description)\n",
        "                      quote_tweet_location.append(qtweet.user.location)\n",
        "                      quote_tweet_followers.append(qtweet.user.followers_count)\n",
        "                      quote_tweet_following.append(qtweet.user.friends_count)\n",
        "                      quote_tweet_place.append(qtweet.place.country)\n",
        "                      \n",
        "                      \n",
        "                      \n",
        "                      #extract hashtags\n",
        "                      hashtags = qtweet.entities['hashtags']\n",
        "                      hashtext = list()\n",
        "                      for i in range(0, len(hashtags)):\n",
        "                        hashtext.append(hashtags[i]['text'])\n",
        "                      quote_tweet_hashtags.append(hashtext)\n",
        "\n",
        "  \n",
        "    #DATAFRAME TO STORE TWEET INFORMATION            \n",
        "    \n",
        "    df_tweets = pd.DataFrame({'tweet_id': tweet_id, 'user_name':tweet_user, 'user_description': tweet_description, \"total_user_tweets\": tweet_totaltweets, 'time': tweet_time, 'tweet': tweet_string, 'location' : tweet_location, 'followers': tweet_followers,\n",
        "          'following': tweet_following,'hashtags': tweet_hashtags,'likes': tweet_like, 'number_of_retweets': tweet_retweet_count, \n",
        "          'tweet_place' : tweet_place })\n",
        "    df_tweets.to_csv(f\"{key_word}.csv\")\n",
        "\n",
        "    #DATAFRAME TO STORE RETWEET INFORMATION PER TWEET\n",
        "\n",
        "    df_retweets = pd.DataFrame({'retweet_id' : retweet_id, 'retweet_text' : retweet_string, 'name' :retweet_user, 'time': retweet_time,'location' : retweet_location, 'followers': retweet_followers,\n",
        "                        'following': retweet_following, 'hashtags': retweet_hashtags,  'retweet_place' : retweet_place})    \n",
        "    df_retweets.to_csv(f\"{key_word}_retweets.csv\") \n",
        "\n",
        "    #DATAFRAME TO STORE QUOTE_TWEET INFORMATION PER TWEET \n",
        "  \n",
        "    df_qtweets = pd.DataFrame({'quote_tweet_id' : quote_tweet_id, 'quote_tweet_text' : quote_tweet_string, 'name':quote_tweet_user, 'time': quote_tweet_time,'location' : quote_tweet_location, 'followers': quote_tweet_followers,\n",
        "                  'following': quote_tweet_following, 'hashtags': quote_tweet_hashtags, 'quote_tweet_place' : quote_tweet_place\n",
        "                      })    \n",
        "    df_qtweets.to_csv(f\"{key_word}_quote_tweets.csv\")\n",
        "\n",
        "    \n",
        "    return df_tweets, df_retweets, df_qtweets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SELA9i7QZrsp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords_list = []\n",
        "\n",
        "file_keywords = pd.read_csv (r'/content/keywords.csv')\n",
        "for i in range(0, len(file_keywords)):\n",
        "  keyword = file_keywords.iloc[i, 1]\n",
        "  keywords_list.append(keyword.lower())\n"
      ],
      "metadata": {
        "id": "sKzzmZVRKZyh",
        "outputId": "f17c1630-0277-4caa-c6b0-67ca0d534f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4233ae53c358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkeywords_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfile_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mr'/content/keywords.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mkeyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_keywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/keywords.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in tqdm(keywords_list): \n",
        "  # Call the function to get related tweets in csv files\n",
        "  df_tweets, df_retweets, df_qtweets = get_related_tweets(keyword, date_since)"
      ],
      "metadata": {
        "id": "qseKSN8F0bKS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "4265d86e-850c-44b7-8db1-ae022051b0cb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:00,  9.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-7c698920b9e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_retweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_qtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_related_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"culture\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_since\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-1af36791f6e7>\u001b[0m in \u001b[0;36mget_related_tweets\u001b[0;34m(key_word, date_since)\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mtweet_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mtweet_location\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mtweet_place\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0mtweet_coordinates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mtweet_followers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfollowers_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(keywords_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwPBUbbOKLYF",
        "outputId": "7088ef6b-7b20-415f-f09b-ad2965d00737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['culture', 'ai', 'artificialintelligence', 'machinelearning', 'deeplearning', 'art', 'arts', 'music', 'cryptocurrency', 'olympics', 'funny', 'crowdfunding', 'influencermarketing', 'opioid', 'healthinsurance', 'photography', 'vegan', 'ethereum', 'bitcoin', 'gold', 'movies', 'running', 'cryptocurrencies', 'java', 'python', 'blockchain', 'datascience', 'fintech', 'science', 'fitnessfacebook', 'youtube', 'amazon', 'weather', 'google', 'gmail', 'instagram', 'translate', 'ipl', 'wordle', 'bbcnews', 'flipkart', 'startbucks', 'cricket', 'fifaworldcup', 'qatar2022', 'tiktok', 'argentina', 'messi', 'worldcup', 'givingtuesday', 'christmas', 'fifaworldcup', 'elections', 'bollywood', 'beinghuman', 'elonmusk', 'dogecoin', 'pokemon', 'nintendoswitch', 'spirituality', 'soul', 'aliens', 'earth', 'water', 'flood', 'naturaldisaster', 'religion', 'god', 'satan', 'entrepreneur', 'hinduism', 'islam', 'christinity', 'jew', 'buddhism', 'home', 'house', 'realestate', 'truth', 'energy', 'racism', 'gratitude', 'wisdom', 'yoga', 'dance', 'football', 'liar', 'famous', 'school', 'college', 'university', 'research', 'billgates', 'elonmusk', 'modi', 'putin', 'russia', 'china', 'india', 'belgium']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "95IuWWm7KLbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mYHHhOX11lZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hnVBdW81lbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaj1A7uY1ldy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JlmLTcaZ1lg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}